{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":85979,"sourceType":"modelInstanceVersion","modelInstanceId":72240,"modelId":76277}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2024 Google LLC.","metadata":{"id":"Tce3stUlHN0L"}},{"cell_type":"code","source":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"cellView":"form","id":"tuOe1ymfHZPu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<table class=\"tfo-notebook-buttons\" align=\"left\">\n  <td>\n    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/pytorch_gemma\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n  <td>\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/pytorch_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/pytorch_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n  </td>\n</table>","metadata":{"id":"aQXQaW_hv5RT"}},{"cell_type":"markdown","source":"# Gemma in PyTorch\n\nThis is a quick demo of running Gemma inference in PyTorch.\nFor more details, please check out the Github repo of the official PyTorch implementation [here](https://github.com/google/gemma_pytorch).\n\n**Note that**:\n * The free Kaggle CPU Python runtime and GPU Python runtime are sufficient for running the Gemma 2B models and 7B int8 quantized models.\n * For advanced use cases for other GPUs or TPU, please refer to [README.md](https://github.com/google/gemma_pytorch/blob/main/README.md) in the official repo.","metadata":{"id":"PXNm5_p_oxMF"}},{"cell_type":"markdown","source":"### Gemma setup\n\nTo complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n\nGemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n\n- Sign in or register at [kaggle.com](https://www.kaggle.com)\n- Open the [Gemma 2 model card](https://www.kaggle.com/models/google/gemma-2) and select _\"Request Access\"_\n- Complete the consent form and accept the terms and conditions\n","metadata":{"id":"jbza6uQdA-0P"}},{"cell_type":"markdown","source":"## Install dependencies","metadata":{"id":"Fqq3HDVfA6Xm"}},{"cell_type":"code","source":"!pip install -q -U torch immutabledict sentencepiece","metadata":{"id":"bMboT70Xop8G","outputId":"42a0d20f-8e74-4772-d9ff-1a52d4daf4bd","execution":{"iopub.status.busy":"2024-08-12T06:29:47.823894Z","iopub.execute_input":"2024-08-12T06:29:47.824187Z","iopub.status.idle":"2024-08-12T06:30:00.485375Z","shell.execute_reply.started":"2024-08-12T06:29:47.824161Z","shell.execute_reply":"2024-08-12T06:30:00.484178Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Download model weights","metadata":{"id":"ENdjDV3nBG5Z"}},{"cell_type":"code","source":"# Choose variant and machine type\nVARIANT = '2b-it' #@param ['2b', '2b-it', '9b', '9b-it', '27b', '27b-it']\nMACHINE_TYPE = 'cuda' #@param ['cuda', 'cpu']\n\nCONFIG = VARIANT[:2]\nif CONFIG == '2b':\n  CONFIG = '2b-v2'","metadata":{"id":"GU5ZZzcZ6ik3","execution":{"iopub.status.busy":"2024-08-12T06:30:00.487545Z","iopub.execute_input":"2024-08-12T06:30:00.487930Z","iopub.status.idle":"2024-08-12T06:30:00.493085Z","shell.execute_reply.started":"2024-08-12T06:30:00.487895Z","shell.execute_reply":"2024-08-12T06:30:00.492048Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"weights_dir = \"/kaggle/input/gemma-2/pytorch/gemma-2-2b-it/1\"","metadata":{"execution":{"iopub.status.busy":"2024-08-12T06:30:00.494919Z","iopub.execute_input":"2024-08-12T06:30:00.495234Z","iopub.status.idle":"2024-08-12T06:30:00.506765Z","shell.execute_reply.started":"2024-08-12T06:30:00.495210Z","shell.execute_reply":"2024-08-12T06:30:00.505950Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\n# Ensure that the tokenizer is present\ntokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\nassert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n\n# Ensure that the checkpoint is present\nckpt_path = os.path.join(weights_dir, f'model.ckpt')\nassert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'","metadata":{"id":"viESUwjq5cAz","execution":{"iopub.status.busy":"2024-08-12T06:31:40.600282Z","iopub.execute_input":"2024-08-12T06:31:40.601006Z","iopub.status.idle":"2024-08-12T06:31:40.608588Z","shell.execute_reply.started":"2024-08-12T06:31:40.600975Z","shell.execute_reply":"2024-08-12T06:31:40.607761Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Download the model implementation","metadata":{"id":"hOft88e7BOBB"}},{"cell_type":"code","source":"# NOTE: The \"installation\" is just cloning the repo.\n!git clone https://github.com/google/gemma_pytorch.git","metadata":{"id":"ww83zI9ToPso","outputId":"f7cea8fb-c052-4f49-e10a-2a5a4412cfe4","execution":{"iopub.status.busy":"2024-08-12T06:31:43.398059Z","iopub.execute_input":"2024-08-12T06:31:43.398757Z","iopub.status.idle":"2024-08-12T06:31:45.187329Z","shell.execute_reply.started":"2024-08-12T06:31:43.398727Z","shell.execute_reply":"2024-08-12T06:31:45.186365Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Cloning into 'gemma_pytorch'...\nremote: Enumerating objects: 239, done.\u001b[K\nremote: Counting objects: 100% (123/123), done.\u001b[K\nremote: Compressing objects: 100% (68/68), done.\u001b[K\nremote: Total 239 (delta 86), reused 58 (delta 55), pack-reused 116\u001b[K\nReceiving objects: 100% (239/239), 2.18 MiB | 25.32 MiB/s, done.\nResolving deltas: 100% (135/135), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\n\nsys.path.append('gemma_pytorch')","metadata":{"id":"sw-KBZ1vBSl3","execution":{"iopub.status.busy":"2024-08-12T06:31:48.365993Z","iopub.execute_input":"2024-08-12T06:31:48.366656Z","iopub.status.idle":"2024-08-12T06:31:48.371588Z","shell.execute_reply.started":"2024-08-12T06:31:48.366623Z","shell.execute_reply":"2024-08-12T06:31:48.370445Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from gemma.config import GemmaConfig, get_model_config\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer\nimport contextlib\nimport os\nimport torch","metadata":{"id":"XFUXlF74BTNe","execution":{"iopub.status.busy":"2024-08-12T06:32:05.032199Z","iopub.execute_input":"2024-08-12T06:32:05.033002Z","iopub.status.idle":"2024-08-12T06:32:06.898988Z","shell.execute_reply.started":"2024-08-12T06:32:05.032970Z","shell.execute_reply":"2024-08-12T06:32:06.898197Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Setup the model","metadata":{"id":"-9PvhVSYBWBt"}},{"cell_type":"code","source":"# Set up model config.\nmodel_config = get_model_config(CONFIG)\nmodel_config.tokenizer = tokenizer_path\nmodel_config.quant = 'quant' in VARIANT\n\n# Instantiate the model and load the weights.\ntorch.set_default_dtype(model_config.get_dtype())\ndevice = torch.device(MACHINE_TYPE)\nmodel = GemmaForCausalLM(model_config)\nmodel.load_weights(ckpt_path)\nmodel = model.to(device).eval()","metadata":{"id":"e2olXB1b45Hz","execution":{"iopub.status.busy":"2024-08-12T06:32:10.185410Z","iopub.execute_input":"2024-08-12T06:32:10.185931Z","iopub.status.idle":"2024-08-12T06:32:33.190710Z","shell.execute_reply.started":"2024-08-12T06:32:10.185902Z","shell.execute_reply":"2024-08-12T06:32:33.189853Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Run inference\n\nBelow are examples for generating in chat mode and generating with multiple\nrequests.\n\nThe instruction-tuned Gemma models were trained with a specific formatter that\nannotates instruction tuning examples with extra information, both during\ntraining and inference. The annotations (1) indicate roles in a conversation,\nand (2) delineate turns in a conversation. Below we show a sample code snippet\nfor formatting the model prompt using the user and model chat templates in a\nmulti-turn conversation. The relevant tokens are:\n\n- `user`: user turn\n- `model`: model turn\n- `<start_of_turn>`: beginning of dialogue turn\n- `<end_of_turn><eos>`: end of dialogue turn\n\nRead about the Gemma formatting for instruction tuning and system instructions\n[here](https://ai.google.dev/gemma/docs/formatting).","metadata":{"id":"738CGmN-BocU"}},{"cell_type":"code","source":"# Generate with one request in chat mode\n\n# Chat templates\nUSER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn><eos>\\n\"\nMODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn><eos>\\n\"\n\n# Sample formatted prompt\nprompt = (\n    USER_CHAT_TEMPLATE.format(\n        prompt='What is a good place for travel in the US?'\n    )\n    + MODEL_CHAT_TEMPLATE.format(prompt='California.')\n    + USER_CHAT_TEMPLATE.format(prompt='What can I do in California?')\n    + '<start_of_turn>model\\n'\n)\nprint('Chat prompt:\\n', prompt)\n\nresults = model.generate(\n    USER_CHAT_TEMPLATE.format(prompt=prompt),\n    device=device,\n    output_len=128,\n)\nprint(results)","metadata":{"id":"yygIK9DEIldp","outputId":"1b3677ee-2b81-46f5-e784-9e3a6784ea01","execution":{"iopub.status.busy":"2024-08-12T06:33:35.254581Z","iopub.execute_input":"2024-08-12T06:33:35.255487Z","iopub.status.idle":"2024-08-12T06:33:41.670999Z","shell.execute_reply.started":"2024-08-12T06:33:35.255455Z","shell.execute_reply":"2024-08-12T06:33:41.670100Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Chat prompt:\n <start_of_turn>user\nWhat is a good place for travel in the US?<end_of_turn><eos>\n<start_of_turn>model\nCalifornia.<end_of_turn><eos>\n<start_of_turn>user\nWhat can I do in California?<end_of_turn><eos>\n<start_of_turn>model\n\nCalifornia is bursting with incredible experiences, so to recommend something truly good for you, I need a bit more info!  \n\nTell me:\n\n* **What type of trip are you looking for?**  Relaxing beach vacation? Adventure-filled hiking trip? Big city excitement? Historical and cultural exploration? Foodie tour? \n* **What time of year are you planning to visit?** This drastically impacts weather and activities. \n* **Who are you traveling with?**  Family? Friends? Solo?  \n* **What's your budget?**  There are amazing options for all budgets, but it\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generate sample\nresults = model.generate(\n    'What is colour of Tangerine ?',\n    device=device,\n    output_len=128,\n)\nprint(results)","metadata":{"id":"oP746yI9PirY","outputId":"bdee9e0c-0bb8-48f1-da61-0cef8c036ac9","execution":{"iopub.status.busy":"2024-08-12T06:34:19.683815Z","iopub.execute_input":"2024-08-12T06:34:19.684174Z","iopub.status.idle":"2024-08-12T06:34:25.678142Z","shell.execute_reply.started":"2024-08-12T06:34:19.684147Z","shell.execute_reply":"2024-08-12T06:34:25.677216Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\n\nTangerines are the small, delicious, sweet orange. They are known for their vibrant orange colour.\n\nSo, the colour of a tangerine is **orange**. \n<end_of_turn>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Learn more\n\nNow that you have learned how to use Gemma in Pytorch, you can explore the many\nother things that Gemma can do in [ai.google.dev/gemma](https://ai.google.dev/gemma).\nSee also these other related resources:\n\n- [Gemma model card](https://ai.google.dev/gemma/docs/model_card)\n- [Gemma C++ Tutorial](https://ai.google.dev/gemma/docs/gemma_cpp)\n- [Gemma formatting and system instructions](https://ai.google.dev/gemma/docs/formatting)","metadata":{"id":"IF7B-3UJHMPd"}}]}